# Case Study: Analyzing usenet Text

Note: Evaluation is turned off in this RMD so that we don't need to include thousands of data files in the repo. 
We encourage future cohorts to clean this up into a minimal subset of data to make the sliders look nicer!

## Objectives 

- Understand the [usenet](https://en.wikipedia.org/wiki/Usenet#:~:text=Usenet%20(%2F%CB%88ju%CB%90z,)%20dial%2Dup%20network%20architecture.&text=Usenet%20resembles%20a%20bulletin%20board,forums%20that%20became%20widely%20used.) data via pre-processing 
- Understand how to apply tf-idf & topic modeling to the usenet data 
- Understand how to apply sentiment analysis and n-gram analysis to usenet data

## 9.1 Pre-processing

```{r 09-setup, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(ggplot2)
```


```{r 09-01, eval = FALSE}
training_folder <- "data/20news-bydate/20news-bydate-train/"

# read all files in the folder into a df
read_folder <- function(infolder) {
  tibble(file = dir(infolder, full.names = TRUE)) %>%
    mutate(text = map(file, read_lines)) %>%
    transmute(id = basename(file), text) %>%
    unnest(text)
}

# apply the read_folder function over each subfoler in the training dir

raw_text <- tibble(folder = dir(training_folder, full.names = TRUE)) %>%
  mutate(folder_out = map(folder, read_folder)) %>%
  # flatten folder_out list col of dataframes into regular dataframe
  unnest(cols = c(folder_out)) %>%
  # create three new vars: newsgroup from the base of the subfolder path, id and text
  transmute(newsgroup = basename(folder), id, text)
```

```{r 09-02, eval = FALSE}
head(raw_text)
```


*Newsgroups have a hierarchy like this: main.sub.subsub. In this dataset there are 20 unique newsgroups*

### Pre-processing text

There are a lot of noisy characters in the text field, like 'From:' or email addresses that will probably not be useful for the analyses so they need to be removed using some RegEx. Empty lines and email signatures will also be filtered out.

```{r 09-03, eval = FALSE}
library(stringr)

# email signatures
# must occur after the first occurrence of an empty line,
# and before the first occurrence of a line starting with --
cleaned_text <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") > 0,
         cumsum(str_detect(text, "^--")) == 0) %>%
  ungroup()

# N = 364,364
```


More cleaning:

```{r 09-04, warning=FALSE, eval = FALSE}
cleaned_text2 <- cleaned_text %>%
  # quotes from other users
  filter(str_detect(text, "^[^>]+[A-Za-z\\d]") | text == "",
         # anything containing "writes"
         !str_detect(text, "writes(:|\\.\\.\\.)$"),
         # anything beginning with "in article"
         !str_detect(text, "^In article <"),
         # two specifically noisy records
         !id %in% c(9704, 9985))
# N = 269,838

head(cleaned_text2)
```

TOKENIZE!

```{r 09-05, eval = FALSE}
library(tidytext)

usenet_words <- cleaned_text2 %>%
  unnest_tokens(word, text) %>%
  # remove numbers
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)

# N = 710,438
```


## 9.2 Words in newsgroups

*tf-idf within newsgroups to see which topics are the more frequent and influential*

First step: get the frequency of words by newsgroup:

```{r 09-06, eval = FALSE}
words_by_newsgroup <- usenet_words %>%
  count(newsgroup, word, sort = TRUE) %>%
  ungroup()
```


```{r 09-07, eval = FALSE}
tf_idf <- words_by_newsgroup %>%
  bind_tf_idf(word, newsgroup, n) %>%
  arrange(desc(tf_idf))

head(tf_idf)
```


VISUALIZE EACH BOARD IN A TOPIC

```{r 09-08, eval = FALSE}
tf_idf %>%
  filter(str_detect(newsgroup, "^talk\\.")) %>%
  group_by(newsgroup) %>%
  slice_max(tf_idf, n = 12) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = newsgroup)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ newsgroup, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

Calculating pair-wise correlations between words within a newsgroup

```{r 09-09, eval = FALSE}
library(widyr)

newsgroup_cors <- words_by_newsgroup %>%
  # newsgroup = items to compare, feature = what link one item to the others = word,
  # value = to do correlation on - n
  pairwise_cor(newsgroup, word, n, sort = TRUE)

head(newsgroup_cors)
```


*Topic Modeling!*

Latent Dirichlet Allocation (LDA) will be used to sort Usenet messages from different newsgroups.

First, create the Document-Term matrix
```{r 09-10, eval = FALSE}
# include only words that occur at least 50 times
word_talk_newsgroups <- usenet_words %>%
  # take only the sci related topics
  filter(str_detect(newsgroup, "^talk")) %>%
  group_by(word) %>%
  mutate(word_total = n()) %>%
  ungroup() %>%
  filter(word_total > 50)

# N = 70,794

# convert into a document-term matrix
# with document names that combines topic.word_n
talk_dtm <- word_talk_newsgroups %>%
  unite(document, newsgroup, id) %>%
  count(document, word) %>%
  cast_dtm(document, word, n)

# Returns Large Document Matrix: 1896 x 662
```

Do the topic modeling!

```{r 09-11, eval = FALSE}
library(topicmodels)

talk_lda <- LDA(talk_dtm, k = 4, control = list(seed = 2016))
```

Visualize the modeling to see if the same newsgroups were formed!

```{r 09-12, eval = FALSE}
talk_lda %>%
  tidy() %>%
  group_by(topic) %>%
  slice_max(beta, n = 8) %>%
  ungroup() %>%
  # reorder each term by beta coef within each topic
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
(skipped gamma distribution visualization)


## 9.3 Sentiment Analysis

*Question 1: how often positive or negative words appeared in the Usenet data?*
*Question 1a: which words contributed the most within each newsgroup?*
*Question 2: what were the most positive/negative messages?*

### Question 2:

```{r 09-13, eval = FALSE}
sentiment_messages <- usenet_words %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(newsgroup, id) %>%
  summarize(sentiment = mean(value),
            words = n()) %>%
  ungroup() %>%
  filter(words >= 5)

sentiment_messages %>%
  arrange(desc(sentiment))
```

Clearly message id 53560 was the most positive in the whole dataset. What was it?!

```{r 09-14, eval = FALSE}
print_message <- function(group, message_id) {
  result <- cleaned_text %>%
    filter(newsgroup == group, id == message_id, text != "")
  
  cat(result$text, sep = "\n")
}

print_message("rec.sport.hockey", 53560)
```


What about the most negative?

```{r 09-15, eval = FALSE}
print_message("rec.sport.hockey", 53907)
```

