[["index.html", "Text Mining with R Book Club Welcome", " Text Mining with R Book Club The R4DS Online Learning Community 2021-11-23 Welcome Welcome to the bookclub! This is a companion for the book Text Mining with R by Julia Silge and David Robinson (O’reilly Media, Inc, copyright 2017, 9781491981658). This companion is available at r4ds.io/tidytext. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["book-chapters.html", "Book Chapters", " Book Chapters The tidy text format Sentiment analysis with tidy data Analyzing word and document frequency: tf-idf Relationships between words: n-grams and correlations Converting to and from non-tidy formats Topic modeling Case study: comparing Twitter archives Case study: mining NASA metadata Case study: analyzing usenet text "],["the-tidy-text-format.html", "Chapter 1 The Tidy Text Format", " Chapter 1 The Tidy Text Format tidy data (Wickham 2014) is: Each variable is a column. Each observation is a row. Each type of observational unit is a table. In the text analysis, the tide text format is a table that contains one token per row. One token: a meaniingful unit of text (e.g., words, n-gram, sentence, or paragraph) tidytext package: keep text data in a tidy format (i.e., Using the tidyverse package for tidy data processing). Other R packages for text-mining or text analysis: tm, quanteda, sentiment, text2vec, etc. Check out the CRAN Task View: Natural Language Processing for R packages of text analysis. "],["contrasting-tidy-text-with-other-data-structures.html", "1.1 Contrasting Tidy Text with Other Data Structures", " 1.1 Contrasting Tidy Text with Other Data Structures String: character vectors (i.e., each letter, words, etc.) Corpus: raw strings annotated with additional metadata and details (i.e, a bag of words) Document-term matrix: a sparse matrix describing a collection of documents with one row for each document and one column for each term (tf-idf in Chapter 3). "],["the-unnest_tokens-function.html", "1.2 The unnest_tokens Function", " 1.2 The unnest_tokens Function text &lt;- c(&quot;Because I could not stop for Death -&quot;, &quot;He kindly stopped for me -&quot;, &quot;The Carriage held but just Ourselves -&quot;, &quot;and Immotality&quot;) text ## [1] &quot;Because I could not stop for Death -&quot; ## [2] &quot;He kindly stopped for me -&quot; ## [3] &quot;The Carriage held but just Ourselves -&quot; ## [4] &quot;and Immotality&quot; We need to put this into a data frame to convert it into a tidy text dataset. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ ggplot2 3.3.5 ✔ purrr 0.3.4 ## ✔ tibble 3.1.6 ✔ dplyr 1.0.7 ## ✔ tidyr 1.1.4 ✔ stringr 1.4.0 ## ✔ readr 2.1.0 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() text_df &lt;- tibble(line=1:4, text=text) text_df ## # A tibble: 4 × 2 ## line text ## &lt;int&gt; &lt;chr&gt; ## 1 1 Because I could not stop for Death - ## 2 2 He kindly stopped for me - ## 3 3 The Carriage held but just Ourselves - ## 4 4 and Immotality Now, we can extract tokens (i.e., words in this example) from the data frame by using the unnest_tokens function. library(tidytext) text_df %&gt;% unnest_tokens(word, text) ## # A tibble: 20 × 2 ## line word ## &lt;int&gt; &lt;chr&gt; ## 1 1 because ## 2 1 i ## 3 1 could ## 4 1 not ## 5 1 stop ## 6 1 for ## 7 1 death ## 8 2 he ## 9 2 kindly ## 10 2 stopped ## 11 2 for ## 12 2 me ## 13 3 the ## 14 3 carriage ## 15 3 held ## 16 3 but ## 17 3 just ## 18 3 ourselves ## 19 4 and ## 20 4 immotality unnest_tokens() function - Other columns, such as the line number each word came from are retained. - Punctuation has been stripped. - By default, unnest_tokens() converts the tokens to lowercase, which makes them easier to compare or combine with other datasets (to_lower=FALSE to turn off this option) "],["example-1-tidying-the-works-of-jane-austen.html", "1.3 Example 1: Tidying the works of Jane Austen", " 1.3 Example 1: Tidying the works of Jane Austen Loading the texts and converting it into a tibble data format: library(janeaustenr) library(dplyr) library(stringr) original_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() original_books ## # A tibble: 73,422 × 4 ## text book linenumber chapter ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 &quot;SENSE AND SENSIBILITY&quot; Sense &amp; Sensibility 1 0 ## 2 &quot;&quot; Sense &amp; Sensibility 2 0 ## 3 &quot;by Jane Austen&quot; Sense &amp; Sensibility 3 0 ## 4 &quot;&quot; Sense &amp; Sensibility 4 0 ## 5 &quot;(1811)&quot; Sense &amp; Sensibility 5 0 ## 6 &quot;&quot; Sense &amp; Sensibility 6 0 ## 7 &quot;&quot; Sense &amp; Sensibility 7 0 ## 8 &quot;&quot; Sense &amp; Sensibility 8 0 ## 9 &quot;&quot; Sense &amp; Sensibility 9 0 ## 10 &quot;CHAPTER 1&quot; Sense &amp; Sensibility 10 1 ## # … with 73,412 more rows Restructuring the one-token-per-row tidytext data format: library(tidytext) tidy_books &lt;- original_books %&gt;% unnest_tokens(word, text) tidy_books ## # A tibble: 725,055 × 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 and ## 3 Sense &amp; Sensibility 1 0 sensibility ## 4 Sense &amp; Sensibility 3 0 by ## 5 Sense &amp; Sensibility 3 0 jane ## 6 Sense &amp; Sensibility 3 0 austen ## 7 Sense &amp; Sensibility 5 0 1811 ## 8 Sense &amp; Sensibility 10 1 chapter ## 9 Sense &amp; Sensibility 10 1 1 ## 10 Sense &amp; Sensibility 13 1 the ## # … with 725,045 more rows Removing the stopwords: Often in text analysis, we will want to remove stop words; stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth in English. We can remove stop words (kept in the tidytext dataset stop_words) with an anti_join(). data(stop_words) tidy_books &lt;- tidy_books %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; We can use them all together, as we have here, or filter() to only use one set of stop words if that is more appropriate for a certain analysis. Using the dplyr::count() to summarize the word frequency results as a tidy data table: tidy_books %&gt;% count(word, sort = TRUE) ## # A tibble: 13,914 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 miss 1855 ## 2 time 1337 ## 3 fanny 862 ## 4 dear 822 ## 5 lady 817 ## 6 sir 806 ## 7 day 797 ## 8 emma 787 ## 9 sister 727 ## 10 house 699 ## # … with 13,904 more rows Visualizing the word frequency results as a plot: library(ggplot2) tidy_books %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 600) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) "],["example-2-the-gutenbergr-package.html", "1.4 Example 2: The gutenbergr package", " 1.4 Example 2: The gutenbergr package Check out the Problem with use of gutenberg_download function library(&quot;gutenbergr&quot;) hgwells &lt;- gutenberg_download(c(35, 36, 5230, 159), mirror = &quot;http://mirrors.xmission.com/gutenberg/&quot;) tidy_hgwells &lt;- hgwells %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; tidy_hgwells %&gt;% count(word, sort = TRUE) ## # A tibble: 11,830 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 time 461 ## 2 people 302 ## 3 door 260 ## 4 heard 249 ## 5 black 232 ## 6 stood 229 ## 7 white 224 ## 8 hand 218 ## 9 kemp 213 ## 10 eyes 210 ## # … with 11,820 more rows bronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767), mirror = &quot;http://mirrors.xmission.com/gutenberg/&quot;) tidy_bronte &lt;- bronte %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; tidy_bronte %&gt;% count(word, sort = TRUE) ## # A tibble: 23,303 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 time 1064 ## 2 miss 854 ## 3 day 826 ## 4 hand 767 ## 5 eyes 713 ## 6 don’t 666 ## 7 night 648 ## 8 heart 638 ## 9 looked 601 ## 10 door 591 ## # … with 23,293 more rows Now, calcuating the words frequencies for the three works library(tidyr) frequency &lt;- bind_rows(mutate(tidy_bronte, author = &quot;Brontë Sisters&quot;), mutate(tidy_hgwells, author = &quot;H.G. Wells&quot;), mutate(tidy_books, author = &quot;Jane Austen&quot;)) %&gt;% mutate(word = str_extract(word, &quot;[a-z&#39;]+&quot;)) %&gt;% count(author, word) %&gt;% group_by(author) %&gt;% mutate(proportion = n / sum(n)) %&gt;% dplyr::select(-n) %&gt;% pivot_wider(names_from = author, values_from = proportion) %&gt;% pivot_longer(`Brontë Sisters`:`H.G. Wells`, names_to = &quot;author&quot;, values_to = &quot;proportion&quot;) frequency ## # A tibble: 57,252 × 4 ## word `Jane Austen` author proportion ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 a 0.00000919 Brontë Sisters 0.0000587 ## 2 a 0.00000919 H.G. Wells 0.0000148 ## 3 aback NA Brontë Sisters 0.00000391 ## 4 aback NA H.G. Wells 0.0000148 ## 5 abaht NA Brontë Sisters 0.00000391 ## 6 abaht NA H.G. Wells NA ## 7 abandon NA Brontë Sisters 0.0000313 ## 8 abandon NA H.G. Wells 0.0000148 ## 9 abandoned 0.00000460 Brontë Sisters 0.0000900 ## 10 abandoned 0.00000460 H.G. Wells 0.000178 ## # … with 57,242 more rows Creating a plot library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor # expect a warning about rows with missing values being removed ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) + geom_abline(color = &quot;gray40&quot;, lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + scale_color_gradient(limits = c(0, 0.001), low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) + facet_wrap(~author, ncol = 2) + theme(legend.position=&quot;none&quot;) + labs(y = &quot;Jane Austen&quot;, x = NULL) ## Warning: Removed 40857 rows containing missing values (geom_point). ## Warning: Removed 40859 rows containing missing values (geom_text). Let’s quantify how similar and different these sets of word frequencies are using a correlation test. How correlated are the word frequencies between Austen and the Brontë sisters, and between Austen and Wells? cor.test(data = frequency[frequency$author == &quot;Brontë Sisters&quot;,], ~ proportion + `Jane Austen`) ## ## Pearson&#39;s product-moment correlation ## ## data: proportion and Jane Austen ## t = 111.09, df = 10345, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7286568 0.7462330 ## sample estimates: ## cor ## 0.7375698 cor.test(data = frequency[frequency$author == &quot;H.G. Wells&quot;,], ~ proportion + `Jane Austen`) ## ## Pearson&#39;s product-moment correlation ## ## data: proportion and Jane Austen ## t = 36.083, df = 6046, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3999815 0.4414612 ## sample estimates: ## cor ## 0.4209414 "],["a-flowchart-of-a-typical-text-analysis-using-tidy-data-priciples..html", "1.5 A flowchart of a typical text analysis using tidy data priciples.", " 1.5 A flowchart of a typical text analysis using tidy data priciples. "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log ADD LOG HERE "],["sentiment-analysis-with-tidy-data.html", "Chapter 2 Sentiment analysis with tidy data ", " Chapter 2 Sentiment analysis with tidy data "],["meeting-videos-1.html", "2.1 Meeting Videos", " 2.1 Meeting Videos 2.1.1 Cohort 1 Meeting chat log ADD LOG HERE "],["analyzing-word-and-document-frequency-tf-idf.html", "Chapter 3 Analyzing word and document frequency: tf-idf ", " Chapter 3 Analyzing word and document frequency: tf-idf "],["meeting-videos-2.html", "3.1 Meeting Videos", " 3.1 Meeting Videos 3.1.1 Cohort 1 Meeting chat log ADD LOG HERE "],["relationships-between-words-n-grams-correlations.html", "Chapter 4 Relationships between words: n-grams &amp; correlations ", " Chapter 4 Relationships between words: n-grams &amp; correlations "],["objectives.html", "4.1 Objectives:", " 4.1 Objectives: Understand how to extract relationships between words Understand how text analyses examine which words tend to follow others Understand how to analyse text to determine which words tend to co-occur "],["tokeninzing-by-n-grams.html", "4.2 Tokeninzing by n-grams", " 4.2 Tokeninzing by n-grams n-gram: pair of adjacent words. Useful for identifying frequencies in which certain words appear together so that a model of their relationship can be built. library(dplyr) library(tidytext) library(janeaustenr) # utilize unnest_tokens() however specify the token is &quot;ngram&quot; and n instead of by words austen_bigrams &lt;- austen_books() %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) head(austen_bigrams) ## # A tibble: 6 × 2 ## book bigram ## &lt;fct&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility sense and ## 2 Sense &amp; Sensibility and sensibility ## 3 Sense &amp; Sensibility &lt;NA&gt; ## 4 Sense &amp; Sensibility by jane ## 5 Sense &amp; Sensibility jane austen ## 6 Sense &amp; Sensibility &lt;NA&gt; OK, how about again with some real data: # from Kaggle: https://www.kaggle.com/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv covid_tweets &lt;- readr::read_csv(&quot;data/Corona_NLP_train.csv&quot;) ## Rows: 41157 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): Location, TweetAt, OriginalTweet, Sentiment ## dbl (2): UserName, ScreenName ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. covid_bigrams &lt;- covid_tweets %&gt;% select(OriginalTweet, Sentiment) %&gt;% unnest_tokens(bigram, OriginalTweet, token = &quot;ngrams&quot;, n = 2) head(covid_bigrams) ## # A tibble: 6 × 2 ## Sentiment bigram ## &lt;chr&gt; &lt;chr&gt; ## 1 Neutral menyrbie phil_gahan ## 2 Neutral phil_gahan chrisitv ## 3 Neutral chrisitv https ## 4 Neutral https t.co ## 5 Neutral t.co ifz9fan2pa ## 6 Neutral ifz9fan2pa and This output clearly needs to be filtered covid_bigrams %&gt;% count(bigram, sort = TRUE) ## # A tibble: 474,761 × 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 https t.co 23953 ## 2 covid 19 11687 ## 3 grocery store 4775 ## 4 to the 3873 ## 5 in the 3639 ## 6 of the 3046 ## 7 the coronavirus 2174 ## 8 the grocery 2138 ## 9 the supermarket 1890 ## 10 coronavirus https 1825 ## # … with 474,751 more rows Filter stop-words. stop-words: uninteresting or common words such as “of”, “the”, “be” In order to filter out stop words, we need to separate out the bigrams into separate columns using the separate() function from tidyr. library(tidyr) bigrams_separated &lt;- covid_bigrams %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) # n = 1,275,993 to n = 393,315 bigrams_filtered &lt;- bigrams_separated %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) # new bigram counts: bigram_counts &lt;- bigrams_filtered %&gt;% count(word1, word2, sort = TRUE) # n = 216,367 Clearly there are a lot of people posting links on Twitter (t.co) because of the shortened URLs. Now that we’ve filtered out the stopwords, let’s unite the words to create more true bigrams (no stopwords) again. bigrams_united &lt;- bigrams_filtered %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) bigrams_united ## # A tibble: 393,315 × 2 ## Sentiment bigram ## &lt;chr&gt; &lt;chr&gt; ## 1 Neutral menyrbie phil_gahan ## 2 Neutral phil_gahan chrisitv ## 3 Neutral chrisitv https ## 4 Neutral https t.co ## 5 Neutral t.co ifz9fan2pa ## 6 Neutral https t.co ## 7 Neutral t.co xx6ghgfzcc ## 8 Neutral https t.co ## 9 Neutral t.co i2nlzdxno8 ## 10 Positive advice talk ## # … with 393,305 more rows IF we were interested in trigrams, we can repeat the sequence with n=3 covid_tweets %&gt;% select(OriginalTweet, Sentiment) %&gt;% unnest_tokens(trigram, OriginalTweet, token = &quot;ngrams&quot;, n = 3) %&gt;% separate(trigram, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word3 %in% stop_words$word) %&gt;% count(word1, word2, word3, sort = TRUE) ## # A tibble: 187,821 × 4 ## word1 word2 word3 n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 coronavirus https t.co 1822 ## 2 covid 19 pandemic 917 ## 3 covid19 https t.co 551 ## 4 19 https t.co 512 ## 5 covid 19 https 508 ## 6 grocery store workers 432 ## 7 covid 19 outbreak 386 ## 8 covid 19 crisis 385 ## 9 covid_19 https t.co 365 ## 10 pandemic https t.co 363 ## # … with 187,811 more rows 4.2.1 Analyzing bigrams This dataset does not really give us a grouping variable like the Austen data but they do include sentiment. Let’s try grouping by sentiments the curators have determined the tweets to be to get the which words are most associated with “shopping”. bigrams_filtered %&gt;% filter(word2 == &quot;shopping&quot;) %&gt;% count(Sentiment, word1, sort = TRUE) ## # A tibble: 571 × 3 ## Sentiment word1 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Positive online 398 ## 2 Neutral online 305 ## 3 Negative online 298 ## 4 Extremely Positive online 249 ## 5 Positive grocery 121 ## 6 Extremely Negative online 102 ## 7 Neutral grocery 79 ## 8 Negative grocery 56 ## 9 Extremely Positive grocery 44 ## 10 Negative panic 39 ## # … with 561 more rows Bigrams can be used treated like documents. We can look at the tf-idf and visualize based on sentiment. bigram_tf_idf &lt;- bigrams_united %&gt;% count(Sentiment, bigram) %&gt;% bind_tf_idf(bigram, Sentiment, n) %&gt;% arrange(desc(tf_idf)) bigram_tf_idf ## # A tibble: 252,318 × 6 ## Sentiment bigram n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Extremely Negative price war 57 0.00106 0.511 0.000539 ## 2 Extremely Negative stop panic 110 0.00204 0.223 0.000455 ## 3 Extremely Positive strong amp 17 0.000255 1.61 0.000411 ## 4 Extremely Negative terroristic threats 13 0.000241 1.61 0.000387 ## 5 Extremely Positive experiencing hardships 14 0.000210 1.61 0.000338 ## 6 Extremely Negative walmart trader 19 0.000352 0.916 0.000322 ## 7 Extremely Negative food shortages 34 0.000630 0.511 0.000322 ## 8 Extremely Negative break eggs 10 0.000185 1.61 0.000298 ## 9 Extremely Negative milk break 10 0.000185 1.61 0.000298 ## 10 Extremely Positive friends safe 12 0.000180 1.61 0.000290 ## # … with 252,308 more rows Visualizing tf-idf library(forcats) library(ggplot2) bigram_tf_idf %&gt;% group_by(Sentiment) %&gt;% slice_max(tf_idf, n = 15) %&gt;% ungroup() %&gt;% ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = Sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~Sentiment, ncol = 2, scales = &quot;free&quot;) + labs(x = &quot;tf-idf&quot;, y = NULL) Takeaway- bigrams are informative and can make tokens more understandable they do make the counts more sparse (a two-word pair is more rare). These can be useful in very large datasets 4.2.2 Using bigrams to provide context in sentiment analysis This dataset already contains sentiment of the overall tweet but as we saw in the tf-idf visual, they don’t really make much sense in context of just the bigram. So, let’s re-do it. This could make a difference given the context, such as the usage of “not” before “happy”. bigrams_separated %&gt;% filter(word1 == &quot;not&quot;) %&gt;% count(word1, word2, sort = TRUE) ## # A tibble: 1,135 × 3 ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 not to 220 ## 2 not a 181 ## 3 not be 177 ## 4 not the 149 ## 5 not only 111 ## 6 not going 86 ## 7 not just 86 ## 8 not have 73 ## 9 not panic 70 ## 10 not sure 69 ## # … with 1,125 more rows AFINN will be used to assign a numeric value for each word associated with “not”. Note: You need to run get_sentiments interactively (to approve the download) per licensing requirements, so we can’t show those results in this online version. AFINN &lt;- get_sentiments(&quot;afinn&quot;) # get the most frequent words preceded by &quot;not&quot; not_words &lt;- bigrams_separated %&gt;% filter(word1 == &quot;not&quot;) %&gt;% inner_join(AFINN, by = c(word2 = &quot;word&quot;)) %&gt;% count(word2, value, sort = TRUE) # n = 194 not_words The most common sentiment-associated word following “not” is “panic”. Panic is pretty negative but NOT panic can be more positive. Computing how influential the certain words were in understanding the context in the wrong direction. This is done in the book by multiplying the their frequency by their sentiment value. not_words %&gt;% mutate(contribution = n * value) %&gt;% arrange(desc(abs(contribution))) %&gt;% head(20) %&gt;% mutate(word2 = reorder(word2, contribution)) %&gt;% ggplot(aes(n * value, word2, fill = n * value &gt; 0)) + geom_col(show.legend = FALSE) + labs(x = &quot;Sentiment value * number of occurrences&quot;, y = &quot;Words preceded by \\&quot;not\\&quot;&quot;) Panic looks very influential. Let’s try again with more negation terms negation_words &lt;- c(&quot;not&quot;, &quot;no&quot;, &quot;never&quot;, &quot;without&quot;) negated_words &lt;- bigrams_separated %&gt;% filter(word1 %in% negation_words) %&gt;% inner_join(AFINN, by = c(word2 = &quot;word&quot;)) %&gt;% count(word1, word2, value, sort = TRUE) # n=342 negated_words %&gt;% mutate(contribution = n * value) %&gt;% arrange(desc(abs(contribution))) %&gt;% head(20) %&gt;% mutate(word2 = reorder(word2, contribution)) %&gt;% ggplot(aes(n * value, word2, fill = n * value &gt; 0)) + geom_col(show.legend = FALSE) + facet_wrap(~word1, ncol = 2, scales = &quot;free&quot;) + labs(x = &quot;Sentiment value * number of occurrences&quot;, y = &quot;Words preceded by \\&quot;not\\&quot;&quot;) 4.2.3 Visualizing network of bigrams with ggraph Relationships between words can be visualized using a node graph. nodes: subject (where the edge is coming from), object (where the edge is going to), edge(association between nodes that have weight) library(igraph) # original counts bigram_counts # filter for only relatively common combinations bigram_graph &lt;- bigram_counts %&gt;% filter(n &gt; 20) %&gt;% graph_from_data_frame() bigram_graph Now that the igraph object has been created, we must plot it with ggraph! set.seed(2017) ggraph(bigram_graph, layout = &quot;fr&quot;) + geom_edge_link() + geom_node_point() + geom_node_text(aes(label = name), vjust = 1, hjust = 1) This is what it would look like: set.seed(2020) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) ggraph(bigram_graph, layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, &#39;inches&#39;)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() This is a visualization of a Markov Chain Markov Chain: common model in text analysis. It is a stochastic model that describes a sequence of possible events where the probability of a subsequent event depends on the state of the previous event. In this case, words are assigned probabilities and then the likelihood of the next word depends on the prior word. For example, in a word generator, if the word is “restaurant”, there is a good chance the following word may be “reservation”. "],["counting-and-correlating-pairs-of-words-with-widyr.html", "4.3 Counting and correlating pairs of words with widyr", " 4.3 Counting and correlating pairs of words with widyr Understanding where words co-occur in documents, even though they may not occur next to each other is another piece of useful information. widyr = a package to make matrix operations, like pairwise counts or correlations between two words in the same document, on tidy data easier. For the purposes of our covid example, lets look at only tweets that are “Extremely Positive” and then group them into chunks of 10 tweets. By the way, these tweets span between 03/01/2020 to 04/01/2020. It would have probably been better to convert the TweetAt variable into a date and sort so that we can have chunks in chronological order… covid_chunk_words &lt;- covid_tweets %&gt;% filter(Sentiment == &quot;Extremely Positive&quot;) %&gt;% mutate(chunk = row_number() %/% 10) %&gt;% filter(chunk &gt; 0) %&gt;% unnest_tokens(word, OriginalTweet) %&gt;% filter(!word %in% stop_words$word) covid_chunk_words ## # A tibble: 123,673 × 7 ## UserName ScreenName Location TweetAt Sentiment chunk word ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 share ## 2 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 65 ## 3 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 living ## 4 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 struggling ## 5 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 2 ## 6 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 local ## 7 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 supermarket ## 8 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 due ## 9 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 issues ## 10 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 19 ## # … with 123,663 more rows library(widyr) # count words co-occuring within sections word_pairs &lt;- covid_chunk_words %&gt;% pairwise_count(word, chunk, sort = TRUE) ## Warning: `distinct_()` was deprecated in dplyr 0.7.0. ## Please use `distinct()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. word_pairs ## # A tibble: 9,056,928 × 3 ## item1 item2 n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 t.co https 653 ## 2 https t.co 653 ## 3 https coronavirus 639 ## 4 t.co coronavirus 639 ## 5 coronavirus https 639 ## 6 coronavirus t.co 639 ## 7 covid 19 621 ## 8 https 19 621 ## 9 t.co 19 621 ## 10 19 covid 621 ## # … with 9,056,918 more rows Most common pair of words are links, but outside of those is “covid-19” and “store” word_pairs %&gt;% filter(item1 == &quot;store&quot;) ## # A tibble: 19,409 × 3 ## item1 item2 n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 store https 543 ## 2 store t.co 543 ## 3 store coronavirus 538 ## 4 store 19 521 ## 5 store covid 514 ## 6 store grocery 492 ## 7 store supermarket 431 ## 8 store amp 412 ## 9 store food 403 ## 10 store prices 389 ## # … with 19,399 more rows 4.3.1 Pairwise correlation # we need to filter for at least relatively common words first word_cors &lt;- covid_chunk_words %&gt;% group_by(word) %&gt;% filter(n() &gt;= 20) %&gt;% pairwise_cor(word, chunk, sort = TRUE) word_cors ## # A tibble: 761,256 × 3 ## item1 item2 correlation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 t.co https 1 ## 2 https t.co 1 ## 3 paper toilet 0.857 ## 4 toilet paper 0.857 ## 5 covid 19 0.841 ## 6 19 covid 0.841 ## 7 doctors nurses 0.819 ## 8 nurses doctors 0.819 ## 9 sanitizer hand 0.783 ## 10 hand sanitizer 0.783 ## # … with 761,246 more rows Let’s find words most correlated to “store” word_cors %&gt;% filter(item1 == &quot;store&quot;) ## # A tibble: 872 × 3 ## item1 item2 correlation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 store grocery 0.649 ## 2 store employees 0.185 ## 3 store workers 0.171 ## 4 store weeks 0.113 ## 5 store retail 0.110 ## 6 store service 0.110 ## 7 store coronacrisis 0.109 ## 8 store clerks 0.109 ## 9 store community 0.108 ## 10 store 30 0.106 ## # … with 862 more rows word_cors %&gt;% filter(item1 %in% c(&quot;store&quot;, &quot;workers&quot;, &quot;online&quot;, &quot;sanitizer&quot;)) %&gt;% group_by(item1) %&gt;% slice_max(correlation, n = 6) %&gt;% ungroup() %&gt;% mutate(item2 = reorder(item2, correlation)) %&gt;% ggplot(aes(item2, correlation)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~ item1, scales = &quot;free&quot;) + coord_flip() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
