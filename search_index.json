[["index.html", "Text Mining with R Book Club Welcome", " Text Mining with R Book Club The R4DS Online Learning Community 2021-11-23 Welcome Welcome to the bookclub! This is a companion for the book Text Mining with R by Julia Silge and David Robinson (O’reilly Media, Inc, copyright 2017, 9781491981658). This companion is available at r4ds.io/tidytext. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["book-chapters.html", "Book Chapters", " Book Chapters The tidy text format Sentiment analysis with tidy data Analyzing word and document frequency: tf-idf Relationships between words: n-grams and correlations Converting to and from non-tidy formats Topic modeling Case study: comparing Twitter archives Case study: mining NASA metadata Case study: analyzing usenet text "],["chapter-1.-the-tidy-text-format.html", "Chapter 1 CHAPTER 1. The Tidy Text Format", " Chapter 1 CHAPTER 1. The Tidy Text Format tidy data (Wickham 2014) is: Each variable is a column. Each observation is a row. Each type of observational unit is a table. In the text analysis, the tide text format is a table that contains one token per row. One token: a meaniingful unit of text (e.g., words, n-gram, sentence, or paragraph) tidytext package: keep text data in a tidy format (i.e., Using the tidyverse package for tidy data processing). Other R packages for text-mining or text analysis: tm, quanteda, sentiment, text2vec, etc. Check out the CRAN Task View: Natural Language Processing for R packages of text analysis. "],["contrasting-tidy-text-with-other-data-structures.html", "1.1 Contrasting Tidy Text with Other Data Structures", " 1.1 Contrasting Tidy Text with Other Data Structures String: character vectors (i.e., each letter, words, etc.) Corpus: raw strings annotated with additional metadata and details (i.e, a bag of words) Document-term matrix: a sparse matrix describing a collection of documents with one row for each document and one column for each term (tf-idf in Chapter 3). "],["the-unnest_tokens-function.html", "1.2 The unnest_tokens Function", " 1.2 The unnest_tokens Function text &lt;- c(&quot;Because I could not stop for Death -&quot;, &quot;He kindly stopped for me -&quot;, &quot;The Carriage held but just Ourselves -&quot;, &quot;and Immotality&quot;) text ## [1] &quot;Because I could not stop for Death -&quot; ## [2] &quot;He kindly stopped for me -&quot; ## [3] &quot;The Carriage held but just Ourselves -&quot; ## [4] &quot;and Immotality&quot; We need to put this into a data frame to convert it into a tidy text dataset. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ ggplot2 3.3.5 ✔ purrr 0.3.4 ## ✔ tibble 3.1.6 ✔ dplyr 1.0.7 ## ✔ tidyr 1.1.4 ✔ stringr 1.4.0 ## ✔ readr 2.1.0 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() text_df &lt;- tibble(line=1:4, text=text) text_df ## # A tibble: 4 × 2 ## line text ## &lt;int&gt; &lt;chr&gt; ## 1 1 Because I could not stop for Death - ## 2 2 He kindly stopped for me - ## 3 3 The Carriage held but just Ourselves - ## 4 4 and Immotality Now, we can extract tokens (i.e., words in this example) from the data frame by using the unnest_tokens function. library(tidytext) text_df %&gt;% unnest_tokens(word, text) ## # A tibble: 20 × 2 ## line word ## &lt;int&gt; &lt;chr&gt; ## 1 1 because ## 2 1 i ## 3 1 could ## 4 1 not ## 5 1 stop ## 6 1 for ## 7 1 death ## 8 2 he ## 9 2 kindly ## 10 2 stopped ## 11 2 for ## 12 2 me ## 13 3 the ## 14 3 carriage ## 15 3 held ## 16 3 but ## 17 3 just ## 18 3 ourselves ## 19 4 and ## 20 4 immotality unnest_tokens() function - Other columns, such as the line number each word came from are retained. - Punctuation has been stripped. - By default, unnest_tokens() converts the tokens to lowercase, which makes them easier to compare or combine with other datasets (to_lower=FALSE to turn off this option) "],["example-1-tidying-the-works-of-jane-austen.html", "1.3 Example 1: Tidying the works of Jane Austen", " 1.3 Example 1: Tidying the works of Jane Austen Loading the texts and converting it into a tibble data format: library(janeaustenr) library(dplyr) library(stringr) original_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() original_books ## # A tibble: 73,422 × 4 ## text book linenumber chapter ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 &quot;SENSE AND SENSIBILITY&quot; Sense &amp; Sensibility 1 0 ## 2 &quot;&quot; Sense &amp; Sensibility 2 0 ## 3 &quot;by Jane Austen&quot; Sense &amp; Sensibility 3 0 ## 4 &quot;&quot; Sense &amp; Sensibility 4 0 ## 5 &quot;(1811)&quot; Sense &amp; Sensibility 5 0 ## 6 &quot;&quot; Sense &amp; Sensibility 6 0 ## 7 &quot;&quot; Sense &amp; Sensibility 7 0 ## 8 &quot;&quot; Sense &amp; Sensibility 8 0 ## 9 &quot;&quot; Sense &amp; Sensibility 9 0 ## 10 &quot;CHAPTER 1&quot; Sense &amp; Sensibility 10 1 ## # … with 73,412 more rows Restructuring the one-token-per-row tidytext data format: library(tidytext) tidy_books &lt;- original_books %&gt;% unnest_tokens(word, text) tidy_books ## # A tibble: 725,055 × 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 and ## 3 Sense &amp; Sensibility 1 0 sensibility ## 4 Sense &amp; Sensibility 3 0 by ## 5 Sense &amp; Sensibility 3 0 jane ## 6 Sense &amp; Sensibility 3 0 austen ## 7 Sense &amp; Sensibility 5 0 1811 ## 8 Sense &amp; Sensibility 10 1 chapter ## 9 Sense &amp; Sensibility 10 1 1 ## 10 Sense &amp; Sensibility 13 1 the ## # … with 725,045 more rows Removing the stopwords: Often in text analysis, we will want to remove stop words; stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth in English. We can remove stop words (kept in the tidytext dataset stop_words) with an anti_join(). data(stop_words) tidy_books &lt;- tidy_books %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; We can use them all together, as we have here, or filter() to only use one set of stop words if that is more appropriate for a certain analysis. Using the dplyr::count() to summarize the word frequency results as a tidy data table: tidy_books %&gt;% count(word, sort = TRUE) ## # A tibble: 13,914 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 miss 1855 ## 2 time 1337 ## 3 fanny 862 ## 4 dear 822 ## 5 lady 817 ## 6 sir 806 ## 7 day 797 ## 8 emma 787 ## 9 sister 727 ## 10 house 699 ## # … with 13,904 more rows Visualizing the word frequency results as a plot: library(ggplot2) tidy_books %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 600) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) "],["example-2-the-gutenbergr-package.html", "1.4 Example 2: The gutenbergr package", " 1.4 Example 2: The gutenbergr package Check out the Problem with use of gutenberg_download function library(&quot;gutenbergr&quot;) hgwells &lt;- gutenberg_download(c(35, 36, 5230, 159), mirror = &quot;http://mirrors.xmission.com/gutenberg/&quot;) tidy_hgwells &lt;- hgwells %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; tidy_hgwells %&gt;% count(word, sort = TRUE) ## # A tibble: 11,830 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 time 461 ## 2 people 302 ## 3 door 260 ## 4 heard 249 ## 5 black 232 ## 6 stood 229 ## 7 white 224 ## 8 hand 218 ## 9 kemp 213 ## 10 eyes 210 ## # … with 11,820 more rows bronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767), mirror = &quot;http://mirrors.xmission.com/gutenberg/&quot;) tidy_bronte &lt;- bronte %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; tidy_bronte %&gt;% count(word, sort = TRUE) ## # A tibble: 23,303 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 time 1064 ## 2 miss 854 ## 3 day 826 ## 4 hand 767 ## 5 eyes 713 ## 6 don’t 666 ## 7 night 648 ## 8 heart 638 ## 9 looked 601 ## 10 door 591 ## # … with 23,293 more rows Now, calcuating the words frequencies for the three works library(tidyr) frequency &lt;- bind_rows(mutate(tidy_bronte, author = &quot;Brontë Sisters&quot;), mutate(tidy_hgwells, author = &quot;H.G. Wells&quot;), mutate(tidy_books, author = &quot;Jane Austen&quot;)) %&gt;% mutate(word = str_extract(word, &quot;[a-z&#39;]+&quot;)) %&gt;% count(author, word) %&gt;% group_by(author) %&gt;% mutate(proportion = n / sum(n)) %&gt;% dplyr::select(-n) %&gt;% pivot_wider(names_from = author, values_from = proportion) %&gt;% pivot_longer(`Brontë Sisters`:`H.G. Wells`, names_to = &quot;author&quot;, values_to = &quot;proportion&quot;) frequency ## # A tibble: 57,252 × 4 ## word `Jane Austen` author proportion ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 a 0.00000919 Brontë Sisters 0.0000587 ## 2 a 0.00000919 H.G. Wells 0.0000148 ## 3 aback NA Brontë Sisters 0.00000391 ## 4 aback NA H.G. Wells 0.0000148 ## 5 abaht NA Brontë Sisters 0.00000391 ## 6 abaht NA H.G. Wells NA ## 7 abandon NA Brontë Sisters 0.0000313 ## 8 abandon NA H.G. Wells 0.0000148 ## 9 abandoned 0.00000460 Brontë Sisters 0.0000900 ## 10 abandoned 0.00000460 H.G. Wells 0.000178 ## # … with 57,242 more rows Creating a plot library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor # expect a warning about rows with missing values being removed ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) + geom_abline(color = &quot;gray40&quot;, lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + scale_color_gradient(limits = c(0, 0.001), low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) + facet_wrap(~author, ncol = 2) + theme(legend.position=&quot;none&quot;) + labs(y = &quot;Jane Austen&quot;, x = NULL) ## Warning: Removed 40857 rows containing missing values (geom_point). ## Warning: Removed 40859 rows containing missing values (geom_text). Let’s quantify how similar and different these sets of word frequencies are using a correlation test. How correlated are the word frequencies between Austen and the Brontë sisters, and between Austen and Wells? cor.test(data = frequency[frequency$author == &quot;Brontë Sisters&quot;,], ~ proportion + `Jane Austen`) ## ## Pearson&#39;s product-moment correlation ## ## data: proportion and Jane Austen ## t = 111.09, df = 10345, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7286568 0.7462330 ## sample estimates: ## cor ## 0.7375698 cor.test(data = frequency[frequency$author == &quot;H.G. Wells&quot;,], ~ proportion + `Jane Austen`) ## ## Pearson&#39;s product-moment correlation ## ## data: proportion and Jane Austen ## t = 36.083, df = 6046, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3999815 0.4414612 ## sample estimates: ## cor ## 0.4209414 "],["a-flowchart-of-a-typical-text-analysis-using-tidy-data-priciples..html", "1.5 A flowchart of a typical text analysis using tidy data priciples.", " 1.5 A flowchart of a typical text analysis using tidy data priciples. "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log ADD LOG HERE "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
