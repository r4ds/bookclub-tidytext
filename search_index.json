[["index.html", "Text Mining with R Book Club Welcome", " Text Mining with R Book Club The R4DS Online Learning Community 2021-12-12 Welcome Welcome to the bookclub! This is a companion for the book Text Mining with R by Julia Silge and David Robinson (O’reilly Media, Inc, copyright 2017, 9781491981658). This companion is available at r4ds.io/tidytext. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["book-chapters.html", "Book Chapters", " Book Chapters The tidy text format Sentiment analysis with tidy data Analyzing word and document frequency: tf-idf Relationships between words: n-grams and correlations Converting to and from non-tidy formats Topic modeling Case study: comparing Twitter archives Case study: mining NASA metadata Case study: analyzing usenet text "],["the-tidy-text-format.html", "Chapter 1 The Tidy Text Format", " Chapter 1 The Tidy Text Format tidy data (Wickham 2014) is: Each variable is a column. Each observation is a row. Each type of observational unit is a table. In the text analysis, the tide text format is a table that contains one token per row. One token: a meaniingful unit of text (e.g., words, n-gram, sentence, or paragraph) tidytext package: keep text data in a tidy format (i.e., Using the tidyverse package for tidy data processing). Other R packages for text-mining or text analysis: tm, quanteda, sentiment, text2vec, etc. Check out the CRAN Task View: Natural Language Processing for R packages of text analysis. "],["contrasting-tidy-text-with-other-data-structures.html", "1.1 Contrasting Tidy Text with Other Data Structures", " 1.1 Contrasting Tidy Text with Other Data Structures String: character vectors (i.e., each letter, words, etc.) Corpus: raw strings annotated with additional metadata and details (i.e, a bag of words) Document-term matrix: a sparse matrix describing a collection of documents with one row for each document and one column for each term (tf-idf in Chapter 3). "],["the-unnest_tokens-function.html", "1.2 The unnest_tokens Function", " 1.2 The unnest_tokens Function text &lt;- c(&quot;Because I could not stop for Death -&quot;, &quot;He kindly stopped for me -&quot;, &quot;The Carriage held but just Ourselves -&quot;, &quot;and Immotality&quot;) text ## [1] &quot;Because I could not stop for Death -&quot; ## [2] &quot;He kindly stopped for me -&quot; ## [3] &quot;The Carriage held but just Ourselves -&quot; ## [4] &quot;and Immotality&quot; We need to put this into a data frame to convert it into a tidy text dataset. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ ggplot2 3.3.5 ✔ purrr 0.3.4 ## ✔ tibble 3.1.6 ✔ dplyr 1.0.7 ## ✔ tidyr 1.1.4 ✔ stringr 1.4.0 ## ✔ readr 2.1.1 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() text_df &lt;- tibble(line=1:4, text=text) text_df ## # A tibble: 4 × 2 ## line text ## &lt;int&gt; &lt;chr&gt; ## 1 1 Because I could not stop for Death - ## 2 2 He kindly stopped for me - ## 3 3 The Carriage held but just Ourselves - ## 4 4 and Immotality Now, we can extract tokens (i.e., words in this example) from the data frame by using the unnest_tokens function. library(tidytext) text_df %&gt;% unnest_tokens(word, text) ## # A tibble: 20 × 2 ## line word ## &lt;int&gt; &lt;chr&gt; ## 1 1 because ## 2 1 i ## 3 1 could ## 4 1 not ## 5 1 stop ## 6 1 for ## 7 1 death ## 8 2 he ## 9 2 kindly ## 10 2 stopped ## 11 2 for ## 12 2 me ## 13 3 the ## 14 3 carriage ## 15 3 held ## 16 3 but ## 17 3 just ## 18 3 ourselves ## 19 4 and ## 20 4 immotality unnest_tokens() function - Other columns, such as the line number each word came from are retained. - Punctuation has been stripped. - By default, unnest_tokens() converts the tokens to lowercase, which makes them easier to compare or combine with other datasets (to_lower=FALSE to turn off this option) "],["example-1-tidying-the-works-of-jane-austen.html", "1.3 Example 1: Tidying the works of Jane Austen", " 1.3 Example 1: Tidying the works of Jane Austen Loading the texts and converting it into a tibble data format: library(janeaustenr) library(dplyr) library(stringr) original_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() original_books ## # A tibble: 73,422 × 4 ## text book linenumber chapter ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 &quot;SENSE AND SENSIBILITY&quot; Sense &amp; Sensibility 1 0 ## 2 &quot;&quot; Sense &amp; Sensibility 2 0 ## 3 &quot;by Jane Austen&quot; Sense &amp; Sensibility 3 0 ## 4 &quot;&quot; Sense &amp; Sensibility 4 0 ## 5 &quot;(1811)&quot; Sense &amp; Sensibility 5 0 ## 6 &quot;&quot; Sense &amp; Sensibility 6 0 ## 7 &quot;&quot; Sense &amp; Sensibility 7 0 ## 8 &quot;&quot; Sense &amp; Sensibility 8 0 ## 9 &quot;&quot; Sense &amp; Sensibility 9 0 ## 10 &quot;CHAPTER 1&quot; Sense &amp; Sensibility 10 1 ## # … with 73,412 more rows Restructuring the one-token-per-row tidytext data format: library(tidytext) tidy_books &lt;- original_books %&gt;% unnest_tokens(word, text) tidy_books ## # A tibble: 725,055 × 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 and ## 3 Sense &amp; Sensibility 1 0 sensibility ## 4 Sense &amp; Sensibility 3 0 by ## 5 Sense &amp; Sensibility 3 0 jane ## 6 Sense &amp; Sensibility 3 0 austen ## 7 Sense &amp; Sensibility 5 0 1811 ## 8 Sense &amp; Sensibility 10 1 chapter ## 9 Sense &amp; Sensibility 10 1 1 ## 10 Sense &amp; Sensibility 13 1 the ## # … with 725,045 more rows Removing the stopwords: Often in text analysis, we will want to remove stop words; stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth in English. We can remove stop words (kept in the tidytext dataset stop_words) with an anti_join(). data(stop_words) tidy_books &lt;- tidy_books %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; We can use them all together, as we have here, or filter() to only use one set of stop words if that is more appropriate for a certain analysis. Using the dplyr::count() to summarize the word frequency results as a tidy data table: tidy_books %&gt;% count(word, sort = TRUE) ## # A tibble: 13,914 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 miss 1855 ## 2 time 1337 ## 3 fanny 862 ## 4 dear 822 ## 5 lady 817 ## 6 sir 806 ## 7 day 797 ## 8 emma 787 ## 9 sister 727 ## 10 house 699 ## # … with 13,904 more rows Visualizing the word frequency results as a plot: library(ggplot2) tidy_books %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 600) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) "],["example-2-the-gutenbergr-package.html", "1.4 Example 2: The gutenbergr package", " 1.4 Example 2: The gutenbergr package Check out the Problem with use of gutenberg_download function library(&quot;gutenbergr&quot;) hgwells &lt;- gutenberg_download(c(35, 36, 5230, 159), mirror = &quot;http://mirrors.xmission.com/gutenberg/&quot;) tidy_hgwells &lt;- hgwells %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; tidy_hgwells %&gt;% count(word, sort = TRUE) ## # A tibble: 11,811 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 time 461 ## 2 people 302 ## 3 door 260 ## 4 heard 249 ## 5 black 232 ## 6 stood 229 ## 7 white 224 ## 8 hand 218 ## 9 kemp 213 ## 10 eyes 210 ## # … with 11,801 more rows bronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767), mirror = &quot;http://mirrors.xmission.com/gutenberg/&quot;) tidy_bronte &lt;- bronte %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; tidy_bronte %&gt;% count(word, sort = TRUE) ## # A tibble: 23,303 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 time 1064 ## 2 miss 854 ## 3 day 826 ## 4 hand 767 ## 5 eyes 713 ## 6 don’t 666 ## 7 night 648 ## 8 heart 638 ## 9 looked 601 ## 10 door 591 ## # … with 23,293 more rows Now, calcuating the words frequencies for the three works library(tidyr) frequency &lt;- bind_rows(mutate(tidy_bronte, author = &quot;Brontë Sisters&quot;), mutate(tidy_hgwells, author = &quot;H.G. Wells&quot;), mutate(tidy_books, author = &quot;Jane Austen&quot;)) %&gt;% mutate(word = str_extract(word, &quot;[a-z&#39;]+&quot;)) %&gt;% count(author, word) %&gt;% group_by(author) %&gt;% mutate(proportion = n / sum(n)) %&gt;% dplyr::select(-n) %&gt;% pivot_wider(names_from = author, values_from = proportion) %&gt;% pivot_longer(`Brontë Sisters`:`H.G. Wells`, names_to = &quot;author&quot;, values_to = &quot;proportion&quot;) frequency ## # A tibble: 57,128 × 4 ## word `Jane Austen` author proportion ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 a 0.00000919 Brontë Sisters 0.0000587 ## 2 a 0.00000919 H.G. Wells 0.0000147 ## 3 aback NA Brontë Sisters 0.00000391 ## 4 aback NA H.G. Wells 0.0000147 ## 5 abaht NA Brontë Sisters 0.00000391 ## 6 abaht NA H.G. Wells NA ## 7 abandon NA Brontë Sisters 0.0000313 ## 8 abandon NA H.G. Wells 0.0000147 ## 9 abandoned 0.00000460 Brontë Sisters 0.0000900 ## 10 abandoned 0.00000460 H.G. Wells 0.000177 ## # … with 57,118 more rows Creating a plot library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor # expect a warning about rows with missing values being removed ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) + geom_abline(color = &quot;gray40&quot;, lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + scale_color_gradient(limits = c(0, 0.001), low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) + facet_wrap(~author, ncol = 2) + theme(legend.position=&quot;none&quot;) + labs(y = &quot;Jane Austen&quot;, x = NULL) ## Warning: Removed 40771 rows containing missing values (geom_point). ## Warning: Removed 40773 rows containing missing values (geom_text). Let’s quantify how similar and different these sets of word frequencies are using a correlation test. How correlated are the word frequencies between Austen and the Brontë sisters, and between Austen and Wells? cor.test(data = frequency[frequency$author == &quot;Brontë Sisters&quot;,], ~ proportion + `Jane Austen`) ## ## Pearson&#39;s product-moment correlation ## ## data: proportion and Jane Austen ## t = 111.09, df = 10345, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7286568 0.7462330 ## sample estimates: ## cor ## 0.7375698 cor.test(data = frequency[frequency$author == &quot;H.G. Wells&quot;,], ~ proportion + `Jane Austen`) ## ## Pearson&#39;s product-moment correlation ## ## data: proportion and Jane Austen ## t = 35.229, df = 6008, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3925914 0.4345047 ## sample estimates: ## cor ## 0.4137673 "],["a-flowchart-of-a-typical-text-analysis-using-tidy-data-priciples..html", "1.5 A flowchart of a typical text analysis using tidy data priciples.", " 1.5 A flowchart of a typical text analysis using tidy data priciples. "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log ADD LOG HERE "],["sentiment-analysis-with-tidy-data.html", "Chapter 2 Sentiment analysis with tidy data", " Chapter 2 Sentiment analysis with tidy data Learning objectives: Learn how to use tidytext approach to find sentiment of a text Learn about diff sentiment lexicon Learn how to use Wordcloud "],["sentiment-analysis-with-tidy-data-1.html", "2.1 Sentiment analysis with tidy data", " 2.1 Sentiment analysis with tidy data Many approach for sentiment analysis: Lexicon-based : Interpretable result Machine learning based : better performance Figure 2.1: A flowchart of a typical text analysis that uses tidytext for sentiment analysis. This chapter shows how to implement sentiment analysis using tidy data principles. "],["sentimentemotion-lexicons.html", "2.2 Sentiment/emotion Lexicons", " 2.2 Sentiment/emotion Lexicons Sentiment lexicon is a dictionary of word with thier semantic orientation General sentiment lexicon bing : positive and negative nrc : positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust AFINN : Assign scores to words (-5 to +5) Domain-specific Sentiment lexicon performs better, but difficult to generate Lexicon can be Unigram and ngram issue with text with many paragraphs (can often have positive and negative sentiment averaged out to about zero) How are sentiment lexicon created crowdsourcing (using, for example, Amazon Mechanical Turk) by the labor of one of the authors. library(tidyverse) library(tidytext) library(janeaustenr) library(stringr) library(lexicon) Note: These sentiments aren’t shown in the online version of these notes, because they need to be manually downloaded for licensing reasons. head(get_sentiments(&quot;afinn&quot;)) head(get_sentiments(&quot;bing&quot;)) nrc_emotions_lex &lt;- read_tsv(&quot;data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt&quot;, col_names = FALSE) head(nrc_emotions_lex) ## # A tibble: 6 × 3 ## X1 X2 X3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 aback anger 0 ## 2 aback anticipation 0 ## 3 aback disgust 0 ## 4 aback fear 0 ## 5 aback joy 0 ## 6 aback negative 0 # get_sentiments(&quot;nrc&quot;) is not working nrc_emotions_lex &lt;- read_tsv(&quot;data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt&quot;, col_names = FALSE) %&gt;% rename( &quot;word&quot; = 1, &quot;sentiment&quot; = 2, &quot;score&quot; = 3) %&gt;% select(-score) head(nrc_emotions_lex) ## # A tibble: 6 × 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 aback anger ## 2 aback anticipation ## 3 aback disgust ## 4 aback fear ## 5 aback joy ## 6 aback negative head(lexicon::nrc_emotions) ## # A tibble: 6 × 9 ## term anger anticipation disgust fear joy sadness surprise trust ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 aback 0 0 0 0 0 0 0 0 ## 2 abacus 0 0 0 0 0 0 0 1 ## 3 abandon 0 0 0 1 0 1 0 0 ## 4 abandoned 1 0 0 1 0 1 0 0 ## 5 abandonment 1 0 0 1 0 1 1 0 ## 6 abate 0 0 0 0 0 0 0 0 "],["sentiment-analysis-with-inner-join.html", "2.3 Sentiment analysis with inner join", " 2.3 Sentiment analysis with inner join With data in a tidy format, sentiment analysis can be done as an inner join. text mining as a tidy data analysis task; much as removing stop words is an antijoin operation, performing sentiment analysis is an inner join operation. austen_books() ## # A tibble: 73,422 × 2 ## text book ## * &lt;chr&gt; &lt;fct&gt; ## 1 &quot;SENSE AND SENSIBILITY&quot; Sense &amp; Sensibility ## 2 &quot;&quot; Sense &amp; Sensibility ## 3 &quot;by Jane Austen&quot; Sense &amp; Sensibility ## 4 &quot;&quot; Sense &amp; Sensibility ## 5 &quot;(1811)&quot; Sense &amp; Sensibility ## 6 &quot;&quot; Sense &amp; Sensibility ## 7 &quot;&quot; Sense &amp; Sensibility ## 8 &quot;&quot; Sense &amp; Sensibility ## 9 &quot;&quot; Sense &amp; Sensibility ## 10 &quot;CHAPTER 1&quot; Sense &amp; Sensibility ## # … with 73,412 more rows (books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() ) ## # A tibble: 73,422 × 4 ## text book linenumber chapter ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 &quot;SENSE AND SENSIBILITY&quot; Sense &amp; Sensibility 1 0 ## 2 &quot;&quot; Sense &amp; Sensibility 2 0 ## 3 &quot;by Jane Austen&quot; Sense &amp; Sensibility 3 0 ## 4 &quot;&quot; Sense &amp; Sensibility 4 0 ## 5 &quot;(1811)&quot; Sense &amp; Sensibility 5 0 ## 6 &quot;&quot; Sense &amp; Sensibility 6 0 ## 7 &quot;&quot; Sense &amp; Sensibility 7 0 ## 8 &quot;&quot; Sense &amp; Sensibility 8 0 ## 9 &quot;&quot; Sense &amp; Sensibility 9 0 ## 10 &quot;CHAPTER 1&quot; Sense &amp; Sensibility 10 1 ## # … with 73,412 more rows (tidy_books &lt;- books %&gt;% unnest_tokens(words, text)) ## # A tibble: 725,055 × 4 ## book linenumber chapter words ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 and ## 3 Sense &amp; Sensibility 1 0 sensibility ## 4 Sense &amp; Sensibility 3 0 by ## 5 Sense &amp; Sensibility 3 0 jane ## 6 Sense &amp; Sensibility 3 0 austen ## 7 Sense &amp; Sensibility 5 0 1811 ## 8 Sense &amp; Sensibility 10 1 chapter ## 9 Sense &amp; Sensibility 10 1 1 ## 10 Sense &amp; Sensibility 13 1 the ## # … with 725,045 more rows # nrc_joy &lt;- get_sentiments(&quot;nrc&quot;) # does not work nrc_emotions_lex &lt;- read_tsv(&quot;data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt&quot;, col_names = FALSE) %&gt;% rename( &quot;words&quot; = 1, &quot;sentiment&quot; = 2, &quot;score&quot; = 3) %&gt;% select(-score) nrc_joy &lt;- nrc_emotions_lex %&gt;% filter(sentiment == &quot;joy&quot;) nrc_joy ## # A tibble: 14,182 × 2 ## words sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 aback joy ## 2 abacus joy ## 3 abandon joy ## 4 abandoned joy ## 5 abandonment joy ## 6 abate joy ## 7 abatement joy ## 8 abba joy ## 9 abbot joy ## 10 abbreviate joy ## # … with 14,172 more rows tidy_books %&gt;% filter(book == &quot;Emma&quot;)%&gt;% inner_join(nrc_joy) %&gt;% count(words, sort = TRUE) ## Joining, by = &quot;words&quot; ## # A tibble: 3,504 × 2 ## words n ## &lt;chr&gt; &lt;int&gt; ## 1 miss 599 ## 2 thing 397 ## 3 good 359 ## 4 time 279 ## 5 dear 241 ## 6 thought 226 ## 7 man 218 ## 8 frank 200 ## 9 young 192 ## 10 day 186 ## # … with 3,494 more rows "],["examining-how-sentiment-changes-in-each-novel.html", "2.4 Examining how sentiment changes in each novel", " 2.4 Examining how sentiment changes in each novel (tidy_books &lt;- books %&gt;% unnest_tokens(word, text)) ## # A tibble: 725,055 × 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 and ## 3 Sense &amp; Sensibility 1 0 sensibility ## 4 Sense &amp; Sensibility 3 0 by ## 5 Sense &amp; Sensibility 3 0 jane ## 6 Sense &amp; Sensibility 3 0 austen ## 7 Sense &amp; Sensibility 5 0 1811 ## 8 Sense &amp; Sensibility 10 1 chapter ## 9 Sense &amp; Sensibility 10 1 1 ## 10 Sense &amp; Sensibility 13 1 the ## # … with 725,045 more rows jane_austen_sentiment &lt;- tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;% count(book, index = linenumber %/% 80, sentiment) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = positive - negative) jane_austen_sentiment ## # A tibble: 920 × 5 ## book index negative positive sentiment ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Sense &amp; Sensibility 0 16 32 16 ## 2 Sense &amp; Sensibility 1 19 53 34 ## 3 Sense &amp; Sensibility 2 12 31 19 ## 4 Sense &amp; Sensibility 3 15 31 16 ## 5 Sense &amp; Sensibility 4 16 34 18 ## 6 Sense &amp; Sensibility 5 16 51 35 ## 7 Sense &amp; Sensibility 6 24 40 16 ## 8 Sense &amp; Sensibility 7 23 51 28 ## 9 Sense &amp; Sensibility 8 30 40 10 ## 10 Sense &amp; Sensibility 9 15 19 4 ## # … with 910 more rows Now we can plot these sentiment scores across the plot trajectory of each novel. plotting against the index on the x-axis that keeps track of narrative time in sections of text. library(ggplot2) ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) + geom_col(show.legend = FALSE) + facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) "],["comparing-the-three-sentiment-dictionaries.html", "2.5 Comparing the three sentiment dictionaries", " 2.5 Comparing the three sentiment dictionaries Lexicons have diff quality Use three sentiment lexicons and examine how the sentiment changes across the narrative arc of Pride and Prejudice. pride_prejudice &lt;- tidy_books %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) pride_prejudice ## # A tibble: 122,204 × 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Pride &amp; Prejudice 1 0 pride ## 2 Pride &amp; Prejudice 1 0 and ## 3 Pride &amp; Prejudice 1 0 prejudice ## 4 Pride &amp; Prejudice 3 0 by ## 5 Pride &amp; Prejudice 3 0 jane ## 6 Pride &amp; Prejudice 3 0 austen ## 7 Pride &amp; Prejudice 7 1 chapter ## 8 Pride &amp; Prejudice 7 1 1 ## 9 Pride &amp; Prejudice 10 1 it ## 10 Pride &amp; Prejudice 10 1 is ## # … with 122,194 more rows afinn &lt;- pride_prejudice %&gt;% inner_join(get_sentiments(&quot;afinn&quot;)) %&gt;% group_by(index = linenumber %/% 80) %&gt;% summarise(sentiment = sum(value)) %&gt;% mutate(method = &quot;AFINN&quot;) bin_pride_prejudice &lt;- pride_prejudice %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% mutate(method = &quot;Bing et al.&quot;) %&gt;% count(method, index = linenumber %/% 80, sentiment) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = positive - negative) ## Joining, by = &quot;word&quot; bind_rows(afinn, bin_pride_prejudice) %&gt;% ggplot(aes(index, sentiment, fill = method)) + geom_col(show.legend = FALSE) + facet_wrap(~method, ncol = 1, scales = &quot;free_y&quot;) The lexicons for calculating sentiment give results that are different in an absolute sense but have similar relative trajectories through the nove "],["most-common-positive-and-negative-words.html", "2.6 Most common positive and negative words", " 2.6 Most common positive and negative words One advantage of having the data frame with both sentiment and word is that we can analyze word counts that contribute to each sentiment bing_word_counts &lt;- tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% ungroup() bing_word_counts bing_word_counts %&gt;% group_by(sentiment) %&gt;% slice_max(n, n = 10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(x = &quot;Contribution to sentiment&quot;, y = NULL) custom_stop_words &lt;- bind_rows(tibble(word = c(&quot;miss&quot;), lexicon = c(&quot;custom&quot;)), stop_words) custom_stop_words ## # A tibble: 1,150 × 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 miss custom ## 2 a SMART ## 3 a&#39;s SMART ## 4 able SMART ## 5 about SMART ## 6 above SMART ## 7 according SMART ## 8 accordingly SMART ## 9 across SMART ## 10 actually SMART ## # … with 1,140 more rows bing_word_counts %&gt;% anti_join(custom_stop_words, by = &quot;word&quot;) %&gt;% group_by(sentiment) %&gt;% slice_max(n, n = 10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(x = &quot;Contribution to sentiment&quot;, y = NULL) "],["wordclouds.html", "2.7 Wordclouds", " 2.7 Wordclouds library(wordcloud) tidy_books %&gt;% anti_join(stop_words) %&gt;% count(word) %&gt;% with(wordcloud(word, n, max.words = 100)) In other functions, such as comparison.cloud(), you may need to turn the data frame into a matrix with reshape2’s acast() library(reshape2) tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = 100) "],["looking-at-units-beyond-just-words.html", "2.8 Looking at units beyond just words", " 2.8 Looking at units beyond just words Unigram cannot handle Negation e.g I am not having a good day. coreNLP (T. Arnold and Tilton 2016), cleanNLP (T. B. Arnold 2016), and sentimentr (Rinker 2017). bingnegative &lt;- get_sentiments(&quot;bing&quot;) %&gt;% filter(sentiment == &quot;negative&quot;) wordcounts &lt;- tidy_books %&gt;% group_by(book, chapter) %&gt;% summarize(words = n()) ## `summarise()` has grouped output by &#39;book&#39;. You can override using the `.groups` argument. tidy_books %&gt;% semi_join(bingnegative) %&gt;% group_by(book, chapter) %&gt;% summarize(negativewords = n()) %&gt;% left_join(wordcounts, by = c(&quot;book&quot;, &quot;chapter&quot;)) %&gt;% mutate(ratio = negativewords/words) %&gt;% filter(chapter != 0) %&gt;% slice_max(ratio, n = 1) %&gt;% ungroup() ## Joining, by = &quot;word&quot; ## `summarise()` has grouped output by &#39;book&#39;. You can override using the `.groups` argument. ## # A tibble: 6 × 5 ## book chapter negativewords words ratio ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Sense &amp; Sensibility 43 161 3405 0.0473 ## 2 Pride &amp; Prejudice 34 111 2104 0.0528 ## 3 Mansfield Park 46 173 3685 0.0469 ## 4 Emma 15 151 3340 0.0452 ## 5 Northanger Abbey 21 149 2982 0.0500 ## 6 Persuasion 4 62 1807 0.0343 These are the chapters with the most sad words in each book, normalized for number of words in the chapter. "],["meeting-videos-1.html", "2.9 Meeting Videos", " 2.9 Meeting Videos 2.9.1 Cohort 1 Meeting chat log ADD LOG HERE "],["placeholder.html", "Chapter 3 Placeholder ", " Chapter 3 Placeholder "],["meeting-videos-2.html", "3.1 Meeting Videos", " 3.1 Meeting Videos 3.1.1 Cohort 1 Meeting chat log ADD LOG HERE "],["relationships-between-words-n-grams-correlations.html", "Chapter 4 Relationships between words: n-grams &amp; correlations ", " Chapter 4 Relationships between words: n-grams &amp; correlations "],["objectives.html", "4.1 Objectives:", " 4.1 Objectives: Understand how to extract relationships between words Understand how text analyses examine which words tend to follow others Understand how to analyse text to determine which words tend to co-occur "],["tokeninzing-by-n-grams.html", "4.2 Tokeninzing by n-grams", " 4.2 Tokeninzing by n-grams n-gram: pair of adjacent words. Useful for identifying frequencies in which certain words appear together so that a model of their relationship can be built. library(dplyr) library(tidytext) library(janeaustenr) # utilize unnest_tokens() however specify the token is &quot;ngram&quot; and n instead of by words austen_bigrams &lt;- austen_books() %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) head(austen_bigrams) ## # A tibble: 6 × 2 ## book bigram ## &lt;fct&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility sense and ## 2 Sense &amp; Sensibility and sensibility ## 3 Sense &amp; Sensibility &lt;NA&gt; ## 4 Sense &amp; Sensibility by jane ## 5 Sense &amp; Sensibility jane austen ## 6 Sense &amp; Sensibility &lt;NA&gt; OK, how about again with some real data: # from Kaggle: https://www.kaggle.com/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv covid_tweets &lt;- readr::read_csv(&quot;data/Corona_NLP_train.csv&quot;) ## Rows: 41157 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): Location, TweetAt, OriginalTweet, Sentiment ## dbl (2): UserName, ScreenName ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. covid_bigrams &lt;- covid_tweets %&gt;% select(OriginalTweet, Sentiment) %&gt;% unnest_tokens(bigram, OriginalTweet, token = &quot;ngrams&quot;, n = 2) head(covid_bigrams) ## # A tibble: 6 × 2 ## Sentiment bigram ## &lt;chr&gt; &lt;chr&gt; ## 1 Neutral menyrbie phil_gahan ## 2 Neutral phil_gahan chrisitv ## 3 Neutral chrisitv https ## 4 Neutral https t.co ## 5 Neutral t.co ifz9fan2pa ## 6 Neutral ifz9fan2pa and This output clearly needs to be filtered covid_bigrams %&gt;% count(bigram, sort = TRUE) ## # A tibble: 474,761 × 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 https t.co 23953 ## 2 covid 19 11687 ## 3 grocery store 4775 ## 4 to the 3873 ## 5 in the 3639 ## 6 of the 3046 ## 7 the coronavirus 2174 ## 8 the grocery 2138 ## 9 the supermarket 1890 ## 10 coronavirus https 1825 ## # … with 474,751 more rows Filter stop-words. stop-words: uninteresting or common words such as “of”, “the”, “be” In order to filter out stop words, we need to separate out the bigrams into separate columns using the separate() function from tidyr. library(tidyr) bigrams_separated &lt;- covid_bigrams %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) # n = 1,275,993 to n = 393,315 bigrams_filtered &lt;- bigrams_separated %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) # new bigram counts: bigram_counts &lt;- bigrams_filtered %&gt;% count(word1, word2, sort = TRUE) # n = 216,367 Clearly there are a lot of people posting links on Twitter (t.co) because of the shortened URLs. Now that we’ve filtered out the stopwords, let’s unite the words to create more true bigrams (no stopwords) again. bigrams_united &lt;- bigrams_filtered %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) bigrams_united ## # A tibble: 393,315 × 2 ## Sentiment bigram ## &lt;chr&gt; &lt;chr&gt; ## 1 Neutral menyrbie phil_gahan ## 2 Neutral phil_gahan chrisitv ## 3 Neutral chrisitv https ## 4 Neutral https t.co ## 5 Neutral t.co ifz9fan2pa ## 6 Neutral https t.co ## 7 Neutral t.co xx6ghgfzcc ## 8 Neutral https t.co ## 9 Neutral t.co i2nlzdxno8 ## 10 Positive advice talk ## # … with 393,305 more rows IF we were interested in trigrams, we can repeat the sequence with n=3 covid_tweets %&gt;% select(OriginalTweet, Sentiment) %&gt;% unnest_tokens(trigram, OriginalTweet, token = &quot;ngrams&quot;, n = 3) %&gt;% separate(trigram, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word3 %in% stop_words$word) %&gt;% count(word1, word2, word3, sort = TRUE) ## # A tibble: 187,821 × 4 ## word1 word2 word3 n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 coronavirus https t.co 1822 ## 2 covid 19 pandemic 917 ## 3 covid19 https t.co 551 ## 4 19 https t.co 512 ## 5 covid 19 https 508 ## 6 grocery store workers 432 ## 7 covid 19 outbreak 386 ## 8 covid 19 crisis 385 ## 9 covid_19 https t.co 365 ## 10 pandemic https t.co 363 ## # … with 187,811 more rows 4.2.1 Analyzing bigrams This dataset does not really give us a grouping variable like the Austen data but they do include sentiment. Let’s try grouping by sentiments the curators have determined the tweets to be to get the which words are most associated with “shopping”. bigrams_filtered %&gt;% filter(word2 == &quot;shopping&quot;) %&gt;% count(Sentiment, word1, sort = TRUE) ## # A tibble: 571 × 3 ## Sentiment word1 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Positive online 398 ## 2 Neutral online 305 ## 3 Negative online 298 ## 4 Extremely Positive online 249 ## 5 Positive grocery 121 ## 6 Extremely Negative online 102 ## 7 Neutral grocery 79 ## 8 Negative grocery 56 ## 9 Extremely Positive grocery 44 ## 10 Negative panic 39 ## # … with 561 more rows Bigrams can be used treated like documents. We can look at the tf-idf and visualize based on sentiment. bigram_tf_idf &lt;- bigrams_united %&gt;% count(Sentiment, bigram) %&gt;% bind_tf_idf(bigram, Sentiment, n) %&gt;% arrange(desc(tf_idf)) bigram_tf_idf ## # A tibble: 252,318 × 6 ## Sentiment bigram n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Extremely Negative price war 57 0.00106 0.511 0.000539 ## 2 Extremely Negative stop panic 110 0.00204 0.223 0.000455 ## 3 Extremely Positive strong amp 17 0.000255 1.61 0.000411 ## 4 Extremely Negative terroristic threats 13 0.000241 1.61 0.000387 ## 5 Extremely Positive experiencing hardships 14 0.000210 1.61 0.000338 ## 6 Extremely Negative walmart trader 19 0.000352 0.916 0.000322 ## 7 Extremely Negative food shortages 34 0.000630 0.511 0.000322 ## 8 Extremely Negative break eggs 10 0.000185 1.61 0.000298 ## 9 Extremely Negative milk break 10 0.000185 1.61 0.000298 ## 10 Extremely Positive friends safe 12 0.000180 1.61 0.000290 ## # … with 252,308 more rows Visualizing tf-idf library(forcats) library(ggplot2) bigram_tf_idf %&gt;% group_by(Sentiment) %&gt;% slice_max(tf_idf, n = 15) %&gt;% ungroup() %&gt;% ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = Sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~Sentiment, ncol = 2, scales = &quot;free&quot;) + labs(x = &quot;tf-idf&quot;, y = NULL) Takeaway- bigrams are informative and can make tokens more understandable they do make the counts more sparse (a two-word pair is more rare). These can be useful in very large datasets 4.2.2 Using bigrams to provide context in sentiment analysis This dataset already contains sentiment of the overall tweet but as we saw in the tf-idf visual, they don’t really make much sense in context of just the bigram. So, let’s re-do it. This could make a difference given the context, such as the usage of “not” before “happy”. bigrams_separated %&gt;% filter(word1 == &quot;not&quot;) %&gt;% count(word1, word2, sort = TRUE) ## # A tibble: 1,135 × 3 ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 not to 220 ## 2 not a 181 ## 3 not be 177 ## 4 not the 149 ## 5 not only 111 ## 6 not going 86 ## 7 not just 86 ## 8 not have 73 ## 9 not panic 70 ## 10 not sure 69 ## # … with 1,125 more rows AFINN will be used to assign a numeric value for each word associated with “not”. Note: You need to run get_sentiments interactively (to approve the download) per licensing requirements, so we can’t show those results in this online version. AFINN &lt;- get_sentiments(&quot;afinn&quot;) # get the most frequent words preceded by &quot;not&quot; not_words &lt;- bigrams_separated %&gt;% filter(word1 == &quot;not&quot;) %&gt;% inner_join(AFINN, by = c(word2 = &quot;word&quot;)) %&gt;% count(word2, value, sort = TRUE) # n = 194 not_words The most common sentiment-associated word following “not” is “panic”. Panic is pretty negative but NOT panic can be more positive. Computing how influential the certain words were in understanding the context in the wrong direction. This is done in the book by multiplying the their frequency by their sentiment value. not_words %&gt;% mutate(contribution = n * value) %&gt;% arrange(desc(abs(contribution))) %&gt;% head(20) %&gt;% mutate(word2 = reorder(word2, contribution)) %&gt;% ggplot(aes(n * value, word2, fill = n * value &gt; 0)) + geom_col(show.legend = FALSE) + labs(x = &quot;Sentiment value * number of occurrences&quot;, y = &quot;Words preceded by \\&quot;not\\&quot;&quot;) Panic looks very influential. Let’s try again with more negation terms negation_words &lt;- c(&quot;not&quot;, &quot;no&quot;, &quot;never&quot;, &quot;without&quot;) negated_words &lt;- bigrams_separated %&gt;% filter(word1 %in% negation_words) %&gt;% inner_join(AFINN, by = c(word2 = &quot;word&quot;)) %&gt;% count(word1, word2, value, sort = TRUE) # n=342 negated_words %&gt;% mutate(contribution = n * value) %&gt;% arrange(desc(abs(contribution))) %&gt;% head(20) %&gt;% mutate(word2 = reorder(word2, contribution)) %&gt;% ggplot(aes(n * value, word2, fill = n * value &gt; 0)) + geom_col(show.legend = FALSE) + facet_wrap(~word1, ncol = 2, scales = &quot;free&quot;) + labs(x = &quot;Sentiment value * number of occurrences&quot;, y = &quot;Words preceded by \\&quot;not\\&quot;&quot;) 4.2.3 Visualizing network of bigrams with ggraph Relationships between words can be visualized using a node graph. nodes: subject (where the edge is coming from), object (where the edge is going to), edge(association between nodes that have weight) library(igraph) # original counts bigram_counts # filter for only relatively common combinations bigram_graph &lt;- bigram_counts %&gt;% filter(n &gt; 20) %&gt;% graph_from_data_frame() bigram_graph Now that the igraph object has been created, we must plot it with ggraph! set.seed(2017) ggraph(bigram_graph, layout = &quot;fr&quot;) + geom_edge_link() + geom_node_point() + geom_node_text(aes(label = name), vjust = 1, hjust = 1) This is what it would look like: set.seed(2020) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)) ggraph(bigram_graph, layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, &#39;inches&#39;)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() This is a visualization of a Markov Chain Markov Chain: common model in text analysis. It is a stochastic model that describes a sequence of possible events where the probability of a subsequent event depends on the state of the previous event. In this case, words are assigned probabilities and then the likelihood of the next word depends on the prior word. For example, in a word generator, if the word is “restaurant”, there is a good chance the following word may be “reservation”. "],["counting-and-correlating-pairs-of-words-with-widyr.html", "4.3 Counting and correlating pairs of words with widyr", " 4.3 Counting and correlating pairs of words with widyr Understanding where words co-occur in documents, even though they may not occur next to each other is another piece of useful information. widyr = a package to make matrix operations, like pairwise counts or correlations between two words in the same document, on tidy data easier. For the purposes of our covid example, lets look at only tweets that are “Extremely Positive” and then group them into chunks of 10 tweets. By the way, these tweets span between 03/01/2020 to 04/01/2020. It would have probably been better to convert the TweetAt variable into a date and sort so that we can have chunks in chronological order… covid_chunk_words &lt;- covid_tweets %&gt;% filter(Sentiment == &quot;Extremely Positive&quot;) %&gt;% mutate(chunk = row_number() %/% 10) %&gt;% filter(chunk &gt; 0) %&gt;% unnest_tokens(word, OriginalTweet) %&gt;% filter(!word %in% stop_words$word) covid_chunk_words ## # A tibble: 123,673 × 7 ## UserName ScreenName Location TweetAt Sentiment chunk word ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 share ## 2 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 65 ## 3 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 living ## 4 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 struggling ## 5 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 2 ## 6 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 local ## 7 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 supermarket ## 8 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 due ## 9 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 issues ## 10 3845 48797 Drogheda 16-03-2020 Extremely Positive 1 19 ## # … with 123,663 more rows library(widyr) # count words co-occuring within sections word_pairs &lt;- covid_chunk_words %&gt;% pairwise_count(word, chunk, sort = TRUE) ## Warning: `distinct_()` was deprecated in dplyr 0.7.0. ## Please use `distinct()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. word_pairs ## # A tibble: 9,056,928 × 3 ## item1 item2 n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 t.co https 653 ## 2 https t.co 653 ## 3 https coronavirus 639 ## 4 t.co coronavirus 639 ## 5 coronavirus https 639 ## 6 coronavirus t.co 639 ## 7 covid 19 621 ## 8 https 19 621 ## 9 t.co 19 621 ## 10 19 covid 621 ## # … with 9,056,918 more rows Most common pair of words are links, but outside of those is “covid-19” and “store” word_pairs %&gt;% filter(item1 == &quot;store&quot;) ## # A tibble: 19,409 × 3 ## item1 item2 n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 store https 543 ## 2 store t.co 543 ## 3 store coronavirus 538 ## 4 store 19 521 ## 5 store covid 514 ## 6 store grocery 492 ## 7 store supermarket 431 ## 8 store amp 412 ## 9 store food 403 ## 10 store prices 389 ## # … with 19,399 more rows 4.3.1 Pairwise correlation # we need to filter for at least relatively common words first word_cors &lt;- covid_chunk_words %&gt;% group_by(word) %&gt;% filter(n() &gt;= 20) %&gt;% pairwise_cor(word, chunk, sort = TRUE) word_cors ## # A tibble: 761,256 × 3 ## item1 item2 correlation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 t.co https 1 ## 2 https t.co 1 ## 3 paper toilet 0.857 ## 4 toilet paper 0.857 ## 5 covid 19 0.841 ## 6 19 covid 0.841 ## 7 doctors nurses 0.819 ## 8 nurses doctors 0.819 ## 9 sanitizer hand 0.783 ## 10 hand sanitizer 0.783 ## # … with 761,246 more rows Let’s find words most correlated to “store” word_cors %&gt;% filter(item1 == &quot;store&quot;) ## # A tibble: 872 × 3 ## item1 item2 correlation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 store grocery 0.649 ## 2 store employees 0.185 ## 3 store workers 0.171 ## 4 store weeks 0.113 ## 5 store retail 0.110 ## 6 store service 0.110 ## 7 store coronacrisis 0.109 ## 8 store clerks 0.109 ## 9 store community 0.108 ## 10 store 30 0.106 ## # … with 862 more rows word_cors %&gt;% filter(item1 %in% c(&quot;store&quot;, &quot;workers&quot;, &quot;online&quot;, &quot;sanitizer&quot;)) %&gt;% group_by(item1) %&gt;% slice_max(correlation, n = 6) %&gt;% ungroup() %&gt;% mutate(item2 = reorder(item2, correlation)) %&gt;% ggplot(aes(item2, correlation)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~ item1, scales = &quot;free&quot;) + coord_flip() "],["placeholder-1.html", "Chapter 5 Placeholder ", " Chapter 5 Placeholder "],["meeting-videos-3.html", "5.1 Meeting Videos", " 5.1 Meeting Videos 5.1.1 Cohort 1 Meeting chat log ADD LOG HERE "],["placeholder-2.html", "Chapter 6 Placeholder ", " Chapter 6 Placeholder "],["meeting-videos-4.html", "6.1 Meeting Videos", " 6.1 Meeting Videos 6.1.1 Cohort 1 Meeting chat log ADD LOG HERE "],["placeholder-3.html", "Chapter 7 Placeholder ", " Chapter 7 Placeholder "],["meeting-videos-5.html", "7.1 Meeting Videos", " 7.1 Meeting Videos 7.1.1 Cohort 1 Meeting chat log ADD LOG HERE "],["placeholder-4.html", "Chapter 8 Placeholder ", " Chapter 8 Placeholder "],["meeting-videos-6.html", "8.1 Meeting Videos", " 8.1 Meeting Videos 8.1.1 Cohort 1 Meeting chat log ADD LOG HERE "],["case-study-analyzing-usenet-text.html", "Chapter 9 Case Study: Analyzing usenet Text", " Chapter 9 Case Study: Analyzing usenet Text Note: Evaluation is turned off in this RMD so that we don’t need to include thousands of data files in the repo. We encourage future cohorts to clean this up into a minimal subset of data to make the sliders look nicer! "],["objectives-1.html", "9.1 Objectives", " 9.1 Objectives Understand the usenet data via pre-processing Understand how to apply tf-idf &amp; topic modeling to the usenet data Understand how to apply sentiment analysis and n-gram analysis to usenet data "],["pre-processing.html", "9.2 9.1 Pre-processing", " 9.2 9.1 Pre-processing library(dplyr) library(tidyr) library(purrr) library(readr) library(ggplot2) training_folder &lt;- &quot;data/20news-bydate/20news-bydate-train/&quot; # read all files in the folder into a df read_folder &lt;- function(infolder) { tibble(file = dir(infolder, full.names = TRUE)) %&gt;% mutate(text = map(file, read_lines)) %&gt;% transmute(id = basename(file), text) %&gt;% unnest(text) } # apply the read_folder function over each subfoler in the training dir raw_text &lt;- tibble(folder = dir(training_folder, full.names = TRUE)) %&gt;% mutate(folder_out = map(folder, read_folder)) %&gt;% # flatten folder_out list col of dataframes into regular dataframe unnest(cols = c(folder_out)) %&gt;% # create three new vars: newsgroup from the base of the subfolder path, id and text transmute(newsgroup = basename(folder), id, text) head(raw_text) Newsgroups have a hierarchy like this: main.sub.subsub. In this dataset there are 20 unique newsgroups 9.2.1 Pre-processing text There are a lot of noisy characters in the text field, like ‘From:’ or email addresses that will probably not be useful for the analyses so they need to be removed using some RegEx. Empty lines and email signatures will also be filtered out. library(stringr) # email signatures # must occur after the first occurrence of an empty line, # and before the first occurrence of a line starting with -- cleaned_text &lt;- raw_text %&gt;% group_by(newsgroup, id) %&gt;% filter(cumsum(text == &quot;&quot;) &gt; 0, cumsum(str_detect(text, &quot;^--&quot;)) == 0) %&gt;% ungroup() # N = 364,364 More cleaning: cleaned_text2 &lt;- cleaned_text %&gt;% # quotes from other users filter(str_detect(text, &quot;^[^&gt;]+[A-Za-z\\\\d]&quot;) | text == &quot;&quot;, # anything containing &quot;writes&quot; !str_detect(text, &quot;writes(:|\\\\.\\\\.\\\\.)$&quot;), # anything beginning with &quot;in article&quot; !str_detect(text, &quot;^In article &lt;&quot;), # two specifically noisy records !id %in% c(9704, 9985)) # N = 269,838 head(cleaned_text2) TOKENIZE! library(tidytext) usenet_words &lt;- cleaned_text2 %&gt;% unnest_tokens(word, text) %&gt;% # remove numbers filter(str_detect(word, &quot;[a-z&#39;]$&quot;), !word %in% stop_words$word) # N = 710,438 "],["words-in-newsgroups.html", "9.3 9.2 Words in newsgroups", " 9.3 9.2 Words in newsgroups tf-idf within newsgroups to see which topics are the more frequent and influential First step: get the frequency of words by newsgroup: words_by_newsgroup &lt;- usenet_words %&gt;% count(newsgroup, word, sort = TRUE) %&gt;% ungroup() tf_idf &lt;- words_by_newsgroup %&gt;% bind_tf_idf(word, newsgroup, n) %&gt;% arrange(desc(tf_idf)) head(tf_idf) VISUALIZE EACH BOARD IN A TOPIC tf_idf %&gt;% filter(str_detect(newsgroup, &quot;^talk\\\\.&quot;)) %&gt;% group_by(newsgroup) %&gt;% slice_max(tf_idf, n = 12) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, tf_idf)) %&gt;% ggplot(aes(tf_idf, word, fill = newsgroup)) + geom_col(show.legend = FALSE) + facet_wrap(~ newsgroup, scales = &quot;free&quot;) + labs(x = &quot;tf-idf&quot;, y = NULL) Calculating pair-wise correlations between words within a newsgroup library(widyr) newsgroup_cors &lt;- words_by_newsgroup %&gt;% # newsgroup = items to compare, feature = what link one item to the others = word, # value = to do correlation on - n pairwise_cor(newsgroup, word, n, sort = TRUE) head(newsgroup_cors) Topic Modeling! Latent Dirichlet Allocation (LDA) will be used to sort Usenet messages from different newsgroups. First, create the Document-Term matrix # include only words that occur at least 50 times word_talk_newsgroups &lt;- usenet_words %&gt;% # take only the sci related topics filter(str_detect(newsgroup, &quot;^talk&quot;)) %&gt;% group_by(word) %&gt;% mutate(word_total = n()) %&gt;% ungroup() %&gt;% filter(word_total &gt; 50) # N = 70,794 # convert into a document-term matrix # with document names that combines topic.word_n talk_dtm &lt;- word_talk_newsgroups %&gt;% unite(document, newsgroup, id) %&gt;% count(document, word) %&gt;% cast_dtm(document, word, n) # Returns Large Document Matrix: 1896 x 662 Do the topic modeling! library(topicmodels) talk_lda &lt;- LDA(talk_dtm, k = 4, control = list(seed = 2016)) Visualize the modeling to see if the same newsgroups were formed! talk_lda %&gt;% tidy() %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 8) %&gt;% ungroup() %&gt;% # reorder each term by beta coef within each topic mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + scale_y_reordered() (skipped gamma distribution visualization) "],["sentiment-analysis.html", "9.4 9.3 Sentiment Analysis", " 9.4 9.3 Sentiment Analysis Question 1: how often positive or negative words appeared in the Usenet data? Question 1a: which words contributed the most within each newsgroup? Question 2: what were the most positive/negative messages? 9.4.1 Question 2: sentiment_messages &lt;- usenet_words %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(newsgroup, id) %&gt;% summarize(sentiment = mean(value), words = n()) %&gt;% ungroup() %&gt;% filter(words &gt;= 5) sentiment_messages %&gt;% arrange(desc(sentiment)) Clearly message id 53560 was the most positive in the whole dataset. What was it?! print_message &lt;- function(group, message_id) { result &lt;- cleaned_text %&gt;% filter(newsgroup == group, id == message_id, text != &quot;&quot;) cat(result$text, sep = &quot;\\n&quot;) } print_message(&quot;rec.sport.hockey&quot;, 53560) What about the most negative? print_message(&quot;rec.sport.hockey&quot;, 53907) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
